{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A System with PDF\n",
    "\n",
    "This project demonstrates a question-answering (Q&A) system built using LangChain and Groq. It allows you to upload a PDF file, query the contents of the PDF, and receive relevant answers from a powerful large language model (LLM).\n",
    "\n",
    "## How it Works\n",
    "\n",
    "1. **PDF Processing:**\n",
    "   - The project begins by taking a PDF file as input.\n",
    "   - The PDF is loaded and split into smaller chunks of text.\n",
    "   - Each chunk is embedded using Azure OpenAI embeddings.\n",
    "   - The embeddings are then stored in a vector database (FAISS) for efficient similarity search.\n",
    "\n",
    "2. **Question Answering:**\n",
    "   - When a user asks a question, the system performs a similarity search in the vector database using the question.\n",
    "   - The most relevant text chunks from the PDF are retrieved.\n",
    "   - These chunks are passed as context to the Groq LLM (Mixtral-8x7b-32768).\n",
    "   - The LLM generates an answer to the user's question based on the provided context.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.12\n",
    "- `requirements.txt`: Install necessary packages using `pip install -r requirements.txt`.\n",
    "\n",
    "## Environment Variables\n",
    "\n",
    "- **GROQ_API_KEY:** Your Groq API key.\n",
    "- **AZURE_OPENAI_ENDPOINT:** Your Azure OpenAI endpoint.\n",
    "- **MAJNU:** Your Azure OpenAI deployment name.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. **Upload PDF:** Provide a PDF file as input.\n",
    "2. **Ask a Question:**  Enter your question related to the PDF's content.\n",
    "3. **Receive Answer:** The system will generate an answer based on the PDF's content.\n",
    "### Example Interaction\n",
    "\n",
    "- **Document:** SampleSet Assignment (A PDF file uploaded)\n",
    "- **Question:** What are the UI frameworks provided?\n",
    "- **Response:**\n",
    "\n",
    "    For your interactive question-answering (QA) bot interface, you can use the following UI frameworks:\n",
    "\n",
    "    1. **Streamlit** ([streamlit.io](https://streamlit.io/)): Streamlit is an open-source app framework for machine learning and data science projects. It allows you to create interactive web apps with Python in just a few minutes. Streamlit is a great choice for building your QA bot interface due to its simplicity and ease of use. You can use Streamlit to create forms for uploading PDF documents and inputting user queries.\n",
    "\n",
    "    2. **Gradio** ([gradio.app](https://gradio.app/)): Gradio is an open-source library for creating machine learning user interfaces. With Gradio, you can create interactive web interfaces for your models with just a few lines of code. Gradio is an excellent choice for your QA bot interface as it offers features like file uploads, text input, and real-time output rendering.\n",
    "\n",
    "   You can choose either Streamlit or Gradio based on your preferences and familiarity with the framework. Both options should be suitable for your needs.\n",
    "\n",
    "- **Source:** Part 2: Interactive QA Bot Interface \n",
    "Problem Statement: Develop an interactive interface for the QA bot from Part 1, allowing users to input queries and retrieve answers in real time. The interface should...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General RAG Pipeline\n",
    "\n",
    "![Alt text](1.png)\n",
    "\n",
    "The Retrieval-Augmented Generation (RAG) pipeline combines retrieval-based methods with generative models to enhance the process of generating responses based on external documents. It consists of three main stages: Ingestion, Retrieval, and Generation.\n",
    "\n",
    "## 1. Ingestion\n",
    "\n",
    "In this phase, documents are processed to create a searchable format for effective retrieval:\n",
    "\n",
    "- **Documents**: Raw text documents that contain the information to be used.\n",
    "- **Chunks**: The documents are split into smaller, manageable pieces called chunks. This helps in better indexing and retrieval, making the process more efficient.\n",
    "- **Embedding**: Each chunk is transformed into a vector representation using an embedding model. These embeddings capture the semantic meaning of the text and allow for efficient similarity searches.\n",
    "- **Index (Database)**: The embeddings are stored in an index (often a vector database) that supports fast retrieval based on similarity searches.\n",
    "\n",
    "## 2. Retrieval\n",
    "\n",
    "This phase focuses on fetching relevant chunks based on a user query:\n",
    "\n",
    "- **Query**: The user submits a query or question seeking information.\n",
    "- **Index**: The query is processed to generate an embedding, which is then used to search the previously created index.\n",
    "- **Top K Results**: The retrieval system fetches the top K most relevant chunks from the index based on their similarity to the query embedding. This step ensures that only the most pertinent information is considered for response generation.\n",
    "\n",
    "## 3. Generation\n",
    "\n",
    "In this final phase, a response is generated using the retrieved information:\n",
    "\n",
    "- **Top K Results**: The selected top K chunks from the retrieval stage are passed to a generative model (like GPT).\n",
    "- **Response to User**: The generative model uses the context provided by the retrieved chunks to formulate a coherent and relevant response to the user's query. This response can be a summary, answer, or any relevant information extracted and synthesized from the retrieved data.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The RAG pipeline effectively combines retrieval and generation, allowing for more informed and contextually relevant responses by leveraging external knowledge sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlights of Our Document  based Conversational Bot using RAG-üìå\n",
    "![Alt text](2.png)\n",
    "\n",
    "\n",
    "- **Doc-loader**: Loads documents for processing efficiently. üìÑ\n",
    "- **Text-splitter**: Divides text into manageable sections for better analysis. ‚úÇÔ∏è\n",
    "- **Embedding**: Converts text into vector representations for similarity searches. üîç\n",
    "- **Chroma/FAISS DB**: Provides database solutions for storing and retrieving embeddings. üóÑÔ∏è\n",
    "- **Streamlit Client**: Facilitates user interaction with the data retrieval system. üíª\n",
    "- **LLM (Large Language Model)**: Powers natural language understanding and generation. ü§ñ\n",
    "- **Memory Management**: Enhances system performance by storing and recalling previous interactions. üß†\n",
    "\n",
    "## Key Insights -üîë\n",
    "\n",
    "- **Efficient Document Processing**: The use of doc-loaders and text-splitters optimizes the initial handling of large datasets, allowing for quicker access to information. üìÇ\n",
    "- **Vector Representation**: Embeddings transform textual data into vectors, enabling nuanced similarity searches and improving retrieval accuracy. üîó\n",
    "- **Robust Databases**: Chroma and FAISS serve as essential tools for organizing and managing embeddings, ensuring fast retrieval times and scalability. üè¢\n",
    "- **User-Centric Design**: The integration of a Streamlit client allows for a more interactive and user-friendly experience, making complex data accessible. üé®\n",
    "- **Advanced Language Models**: LLMs are crucial for generating coherent responses, highlighting the importance of language understanding in AI applications. üìö\n",
    "- **Dynamic Memory Utilization**: Effective memory management can enhance the system‚Äôs ability to provide contextually relevant information, improving user engagement. üîÑ\n",
    "- **Vector Store Loading**: Loading vector stores efficiently is vital for maintaining performance, particularly in applications requiring real-time data access. ‚è±Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, CSVLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# 1. Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# 2. Initialize LLM with API key\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\", groq_api_key=groq_api_key)\n",
    "\n",
    "# 3. Define the prompt template\n",
    "prompt = PromptTemplate(template=\"Answer the question.\\nQuestion: {question}\\nHelpful Answers:\",\n",
    "                        input_variables=['question'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_name):\n",
    "    # Load the PDF\n",
    "    pdf_loader = PyPDFLoader(file_name)\n",
    "    pdtext = pdf_loader.load()\n",
    "\n",
    "    # Split the loaded text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(pdtext)\n",
    "\n",
    "    # Initialize embeddings and vector store\n",
    "    embeddings = AzureOpenAIEmbeddings(deployment='MAJNU', azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'])\n",
    "    vectordb = FAISS.from_documents(text_chunks, embeddings)\n",
    "\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_file(vectordb, question):\n",
    "    # Perform similarity search on the vector database\n",
    "    docs = vectordb.similarity_search(question, k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Prepare the input for the LLM\n",
    "    input_text = f\"{question}\\n\\nContext: {context}\"\n",
    "    \n",
    "    # Invoke the LLM with the formatted input\n",
    "    response = llm.invoke(input_text)  # Pass the input_text as a string\n",
    "    \n",
    "    # Access the response text correctly\n",
    "    answer = response.content if hasattr(response, 'content') else str(response)  # Adjust based on the response structure\n",
    "    \n",
    "    # Only keep the first source\n",
    "    source = f\"Source: {docs[0].page_content[:200]}...\" if docs else \"No sources available.\"\n",
    "    return answer, source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_files = 'm.pdf'\n",
    "vectordb = process_pdf(uploaded_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: For your interactive QA bot interface, you can use the following UI frameworks:\n",
      "\n",
      "1. Streamlit (<https://streamlit.io/>):\n",
      "Streamlit is an open-source app framework built specifically for machine learning and data science projects. It allows you to create interactive web apps with Python in just a few minutes. Streamlit is a great choice for creating data-intensive and ML-based web applications, and it has built-in support for uploading files, displaying text, and real-time data processing.\n",
      "2. Gradio (<https://gradio.app/>):\n",
      "Gradio is a Python library for creating user interfaces for machine learning models. It enables you to share your models with others by creating user interfaces that are both simple and powerful. Gradio is designed to work with any ML library and supports file uploads, real-time input/output, and multiple input types.\n",
      "\n",
      "For this task, you can choose either Streamlit or Gradio to build the frontend interface. Here's a brief comparison of the two:\n",
      "\n",
      "* Streamlit is more suitable for data-centric applications, while Gradio is more generic and can be used for various types of ML models.\n",
      "* Streamlit has a more opinionated structure and a larger set of built-in components, while Gradio is more flexible and allows you to create custom components easily.\n",
      "* Streamlit is better for integrating data processing and visualization, while Gradio focuses on creating user interfaces for ML models.\n",
      "\n",
      "Considering your requirements, both Streamlit and Gradio can handle file uploads and real-time query processing. You can choose either based on your preference for the framework's structure, built-in components, and customization options.\n",
      "\n",
      "For the integration of the backend from Part 1, you will need to make API calls from the frontend interface (built using Streamlit or Gradio) to the backend to process the uploaded documents, store document embeddings, and provide real-time answers. Ensure that the system can handle multiple queries efficiently and provide accurate, contextually relevant responses.\n",
      "\n",
      "For a successful project, follow the guidelines provided, including using Docker for containerization, handling large documents and multiple queries, sharing the code and deployment instructions through GitHub, and providing a detailed ReadMe file. Additionally, ensure modular and scalable code following best practices for both frontend and backend development, and provide documentation for your approach, decisions, challenges, and solutions.\n",
      "--------------------------------------------------------------------------------\n",
      "Source: Source: Part\n",
      "2:\n",
      "Interactive\n",
      "QA\n",
      "Bot\n",
      "Interface\n",
      "Problem\n",
      "Statement:\n",
      "Develop\n",
      "an\n",
      "interactive\n",
      "interface\n",
      "for\n",
      "the\n",
      "QA\n",
      "bot\n",
      "from\n",
      "Part\n",
      "1,\n",
      "allowing\n",
      "users\n",
      "to\n",
      "input\n",
      "queries\n",
      "and\n",
      "retrieve\n",
      "answers\n",
      "in\n",
      "real\n",
      "time.\n",
      "The\n",
      "interface\n",
      "sh...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sample query\n",
    "question = \"What are the ui framework provided\"\n",
    "answer, source = query_file(vectordb, question)\n",
    "print(\"Answer:\", answer)\n",
    "print('--------------------------------------------------------------------------------')\n",
    "print(\"Source:\", source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
